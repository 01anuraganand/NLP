{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lYEY34jZMveU",
        "h62EHYLwTX1u",
        "3uN98S4DYkcr",
        "csZ9owYUbpop",
        "ZOKDO9WXnCNB",
        "POVo6goO9b5Z",
        "X7t2R4IS_FjZ",
        "1TLrXyFXDtyz",
        "ueVQNwDLICVl",
        "7wryBKS4KVKB"
      ],
      "authorship_tag": "ABX9TyNDjW9y7qHegdVR1VlJPMsw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01anuraganand/NLP/blob/main/Advanced_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPWgrvhgMpp6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Magic Method & Dunder\n",
        "\n",
        "*   https://www.geeksforgeeks.org/dunder-magic-methods-python/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lYEY34jZMveU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person:\n",
        "  def __init__(self, name, age):\n",
        "    self.name = name\n",
        "    self.age = age\n",
        "\n",
        "  def __del__(self):\n",
        "    print(\"Object deconstructed\")\n",
        "p = Person('anurag', 22)\n",
        "del p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKNxnO7wM37_",
        "outputId": "61889cdd-9e38-4938-89c4-ac5a47599e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object deconstructed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Built in classes define many magic methods, dir() function can show you magic methods inherited by a class.\n",
        "print(dir(int)[:10])\n",
        "print(dir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkDBWTWzO4DI",
        "outputId": "5e45e787-bade-4424-8396-523e317c9dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__']\n",
            "['In', 'Out', 'Person', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'quit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vector:\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Vector(self.x + other.x, self.y + other.y)\n",
        "\n",
        "  def __repr__(self): # representation dunder method\n",
        "    \"\"\"\n",
        "    __str__() method. Note: It's true that on the surface, using str() looks similar to using repr() .\n",
        "    However, whereas repr() is a built-in function, str is a class.\n",
        "    Therefore, str() creates an instance of the str class by converting its argument to a string.\n",
        "    \"\"\"\n",
        "    return f\"X: {self.x}, Y: {self.y}\"\n",
        "\n",
        "  def __call__(self):\n",
        "    print(\"Its calling\")\n",
        "\n",
        "v1= Vector(10, 20)\n",
        "v2 = Vector(50, 60)\n",
        "v3 = v1 + v2\n",
        "print(v3.x)\n",
        "print(v3)\n",
        "# call\n",
        "print(v3())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FvRT4tlMyhv",
        "outputId": "40b1d6d4-0ba0-4db2-f72e-1eab79056ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "X: 60, Y: 80\n",
            "Its calling\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator\n",
        "- (lazy execution)"
      ],
      "metadata": {
        "id": "99KkL07dROXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def mygenerator(x):\n",
        "  for i in range(x):\n",
        "    yield i**2\n",
        "\n",
        "values = mygenerator(20)\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "print(\"Size:\", sys.getsizeof(values))\n",
        "for x in values:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAYQuSB-RPox",
        "outputId": "fe2b3ff2-e5e5-48cd-9624-6d4189a5739a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "4\n",
            "9\n",
            "Size: 104\n",
            "16\n",
            "25\n",
            "36\n",
            "49\n",
            "64\n",
            "81\n",
            "100\n",
            "121\n",
            "144\n",
            "169\n",
            "196\n",
            "225\n",
            "256\n",
            "289\n",
            "324\n",
            "361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infinite_sequence():\n",
        "  result = 1\n",
        "  while True:\n",
        "    yield result\n",
        "    result *= 5\n",
        "values = infinite_sequence()\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "print(next(values))\n",
        "\n",
        "# for x in values:\n",
        "#   print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS4PVtk9SKX8",
        "outputId": "9b8d56a4-4629-47c9-b8d1-f91a215a646b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "5\n",
            "25\n",
            "125\n",
            "625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Argument Parsing"
      ],
      "metadata": {
        "id": "h62EHYLwTX1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myfunction(*args, **kwargs):\n",
        "  print(args[0])\n",
        "  print(args[1])\n",
        "  print(args[2])\n",
        "  print(kwargs['KEYONE'])\n",
        "  print(kwargs['KEYTWO'])\n",
        "\n",
        "myfunction('hey', True, 12.2, KEYONE = 'Anurag', KEYTWO = 'Anand')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2FTU1nvTZko",
        "outputId": "9ce2e23b-d444-4aa4-837b-04fac500e646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey\n",
            "True\n",
            "12.2\n",
            "Anurag\n",
            "Anand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.argv[0]) # argv[0] is always the file in which this script is written\n",
        "print(sys.argv[1])\n",
        "print(sys.argv[2])\n",
        "#print(sys.argv[3])  IndexError: list index out of range (based upon system)\n",
        "print(sys.argv) # print whole system argument"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVWkFFBXUOQW",
        "outputId": "4cf9d45a-6a57-4757-f72b-8c76bed2032b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\n",
            "-f\n",
            "/root/.local/share/jupyter/runtime/kernel-92a5db96-e12f-47d6-9539-224dd6b367cc.json\n",
            "['/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-92a5db96-e12f-47d6-9539-224dd6b367cc.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = sys.argv[1]\n",
        "message = sys.argv[2]\n",
        "print(len(sys.argv))\n",
        "with open(filename, 'w+') as f:\n",
        "  f.write(message)\n",
        "#! /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py newfile.txt \"Author : Anurag Anand\"  # not executable in colab due to system permission error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm1ZHJBVUqgY",
        "outputId": "86466bf1-ee99-4ba7-b78f-01e03fe2ef0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional flag (defind as -)\n",
        "import getopt\n",
        "opts, args = getopt.getopt(sys.argv[1:], 'f:m:', ['filename', 'message'])\n",
        "print(opts)\n",
        "print(args)\n",
        "# !'/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py' text.txt Hello\n",
        "for opt, arg in opts:\n",
        "  if opt == '-f':\n",
        "    filename = arg\n",
        "  if opt == '-m':\n",
        "    message = arg\n",
        "\n",
        "with open(filename, 'w+') as f:\n",
        "  f.write(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lvA0tJsVmNm",
        "outputId": "0438d3fc-fe80-4400-c1de-6ee55abd1c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('-f', '/root/.local/share/jupyter/runtime/kernel-92a5db96-e12f-47d6-9539-224dd6b367cc.json')]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_key_args(metrics:list):\n",
        "  if 'key1' in metrics:\n",
        "    print(\"Key1 found\")\n",
        "  elif 'key2' in metrics:\n",
        "    print(\"key2 found\")\n",
        "  else:\n",
        "    print('record you searching not found.')\n",
        "check_key_args(metrics = ['key1', 'key2', 'ra'])"
      ],
      "metadata": {
        "id": "5xy2HWE1Yf7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf294f1d-10d2-468c-d576-a641538c8dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key1 found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encapsulation\n",
        "\n",
        "- In python method overloading is available as in C++"
      ],
      "metadata": {
        "id": "3uN98S4DYkcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the privates var / method we define the getters and setters function\n",
        "class Person:\n",
        "  def __init__(self, name, age, gender):\n",
        "    self.__name = name\n",
        "    self.__age = age\n",
        "    self.__gender = gender\n",
        "\n",
        "  @property # property is used to return the property attributes of a class from the stated getter, setter and deleter as parameters\n",
        "  def Name(self):\n",
        "    return self.__name\n",
        "\n",
        "  @Name.setter\n",
        "  def Name(self, value):\n",
        "    self.__name = value\n",
        "\n",
        "  @staticmethod  # it work on class itself; not on its instance so we pass class name; there is also no requirement to pass the self keyword\n",
        "  def mymethod():\n",
        "    print(\"Calling the static functionality.\")\n",
        "\n",
        "p = Person('anurag', 22, \"M\")\n",
        "print(p.Name)\n",
        "p.Name = \"anand\" # Cant perform if '@Name.setter' dont have setter; through - AttributeError: can't set attribute 'Name'\n",
        "print(p.Name)\n",
        "\n",
        "Person.mymethod()\n",
        "p.mymethod()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlj_vPqrYhVh",
        "outputId": "6a7fd9c5-65db-4600-dbe4-556bdd105a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anurag\n",
            "anand\n",
            "Calling the static functionality.\n",
            "Calling the static functionality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type Hinting"
      ],
      "metadata": {
        "id": "csZ9owYUbpop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import wraps\n",
        "\"\"\"\n",
        "Decorator factory to apply update_wrapper() to a wrapper function\n",
        "\n",
        "Returns a decorator that invokes update_wrapper() with the decorated\n",
        "function as the wrapper argument and the arguments to wraps() as the\n",
        "remaining arguments. Default arguments are as for update_wrapper().\n",
        "This is a convenience function to simplify applying partial() to\n",
        "update_wrapper().\n",
        "\"\"\"\n",
        "from typing import Any, Callable, Type, Union\n",
        "\n",
        "\n",
        "def type_check(func:Callable)->Callable:\n",
        "  @wraps(func)\n",
        "  def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
        "    # check args type\n",
        "    for i, arg in enumerate(args):\n",
        "      expected_type = func.__annotations__.get(f\"arg_{i+1}\", None)\n",
        "      if expected_type is not None and not isinstance(arg, expected_type):\n",
        "        raise TypeError(f'Argument {i+1} must be of type {expected_type.__name__}')\n",
        "\n",
        "    # Check kwargs types\n",
        "    for arg_name, arg_value in kwargs.items():\n",
        "      expected_type = func.__annotations__.get(arg_name, None)\n",
        "      if expected_type is not None and not isinstance(arg_value, expected_type):\n",
        "          raise TypeError(f\"Argument '{arg_name}' must be of type {expected_type.__name__}\")\n",
        "\n",
        "    result  = func(*args, **kwargs)\n",
        "\n",
        "    # check return type\n",
        "    expected_return_type = func.__annotations__.get(\"return\", None)\n",
        "    if expected_return_type is not None and not isinstance(result, expected_return_type):\n",
        "      raise TypeError(f\"Return value must be of type {expected_return_type.__name__}\")\n",
        "    return result\n",
        "\n",
        "  return wrapper\n",
        "\n",
        "\n",
        "@type_check\n",
        "def add(x:int, y:int)->int:\n",
        "  return x + y\n",
        "\n",
        "\n",
        "@type_check\n",
        "def fun1(x:int, y:int)->int:\n",
        "  return str(x + y)\n",
        "\n",
        "@type_check\n",
        "def sub(x, y):\n",
        "  return x - y\n"
      ],
      "metadata": {
        "id": "TqW2cL52bq4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(add.__annotations__)\n",
        "print(add.__annotations__.get('return', None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN2vGE-lhKc8",
        "outputId": "505d4d4b-10e8-41d5-c4f9-1427e62a5a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'x': <class 'int'>, 'y': <class 'int'>, 'return': <class 'int'>}\n",
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(add(5, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTlQL9svYPR1",
        "outputId": "9f3f84f6-a12e-45b4-8384-6f6eed6aa37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Demo by default given by python dynamically\n",
        "#print(add(4, '4'))"
      ],
      "metadata": {
        "id": "rX8Kd1jYRMGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(fun1(5, 5)) # demo by @type_check decorators"
      ],
      "metadata": {
        "id": "2aJ1UmOslKey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factory Design Patterns"
      ],
      "metadata": {
        "id": "ZOKDO9WXnCNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABCMeta, abstractstaticmethod\n",
        "\n",
        "# abstract are those which are used to design the Interface; whichever class is inheriting the abstract class its compulsary to  have abstract function\n",
        "# abstractstaticmethod class are those class which instance can't be created\n",
        "\n",
        "# As python dont have Interface as Java, this is the way we create in python\n",
        "class IPerson(metaclass=ABCMeta):\n",
        "\n",
        "\n",
        "  @abstractstaticmethod\n",
        "  def person_method():\n",
        "    \"\"\"Interface Method\"\"\"\n",
        "\n",
        "#p1 = IPerson() # TypeError: Can't instantiate abstract class IPerson with abstract method person_method\n",
        "#p1.person_method()\n",
        "\n",
        "class Student(IPerson):\n",
        "  def __init__(self):\n",
        "    self.name = 'Basic Student Name'\n",
        "\n",
        "  def person_method(self):\n",
        "    print(\"I'm student\")\n",
        "\n",
        "class Teacher(IPerson):\n",
        "  def __init__(self):\n",
        "    self.name = \"Basic Teacher Name\"\n",
        "\n",
        "  def person_method(self):\n",
        "    print(\"I'm teacher\")\n",
        "\n",
        "s1 = Student()\n",
        "s1.person_method()\n",
        "t1 = Teacher()\n",
        "t1.person_method()\n",
        "\n",
        "class PersonFactory:\n",
        "\n",
        "  @staticmethod\n",
        "  def build_person(person_type):\n",
        "    if person_type == 'Student':\n",
        "      return Student()\n",
        "    if person_type == 'Teacher':\n",
        "      return Teacher()\n",
        "    return -1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  choice = 'Student'\n",
        "  person = PersonFactory.build_person(choice)\n",
        "\n",
        "  # note person wil be returned as the class of Student/Teacher so 'person'-> represent the class on which abatrct method is called\n",
        "  person.person_method()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4aAXMM3jyOM",
        "outputId": "50c22bfc-6c73-4d87-952b-4942e48c9669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm student\n",
            "I'm teacher\n",
            "I'm student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proxy Design Pattern\n",
        "- Similar to decorator\n",
        "- Wrapping functionaly around object creation\n",
        "- Act as middleman while creation of new instance"
      ],
      "metadata": {
        "id": "POVo6goO9b5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABCMeta, abstractstaticmethod\n",
        "\n",
        "class IPerson(metaclass=ABCMeta):\n",
        "  @abstractstaticmethod\n",
        "  def person_method():\n",
        "    \"\"\"Interface Method\"\"\"\n",
        "\n",
        "class Person(IPerson):\n",
        "  def person_method(self):\n",
        "    print(\"I'm Person\")\n",
        "\n",
        "class ProxyPerson(IPerson):\n",
        "  def __init__(self):\n",
        "    self.person = Person()\n",
        "\n",
        "  def person_method(self):\n",
        "    print(\"I'm proxy function.\")\n",
        "    self.person.person_method()\n",
        "\n",
        "p1 = Person()\n",
        "p1.person_method()\n",
        "\n",
        "p2 = ProxyPerson()\n",
        "p2.person_method()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S18T1Tup9xf5",
        "outputId": "d6475bcc-5b26-40e1-c1b1-05c35171627b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm Person\n",
            "I'm proxy function.\n",
            "I'm Person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Singleton Design Pattern\n",
        "- Class having single instance only\n"
      ],
      "metadata": {
        "id": "X7t2R4IS_FjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABCMeta, abstractstaticmethod\n",
        "class IPerson(metaclass=ABCMeta):\n",
        "\n",
        "  @abstractstaticmethod\n",
        "  def print_data():\n",
        "    \"\"\"Implement in child class\"\"\"\n",
        "\n",
        "class PersonSingleton(IPerson):\n",
        "  __instance = None\n",
        "\n",
        "  @staticmethod\n",
        "  def get_instance():\n",
        "    if PersonSingleton.__instance == None:\n",
        "      PersonSingleton(\"Default Name\", 0)\n",
        "\n",
        "    return PersonSingleton.__instance\n",
        "\n",
        "  def __init__(self, name, age):\n",
        "    if PersonSingleton.__instance is not None:\n",
        "      raise Exception(\"Singleton Class cant be re-instantiate\")\n",
        "    else:\n",
        "      self.name = name\n",
        "      self.age = age\n",
        "      PersonSingleton.__instance = self\n",
        "\n",
        "  @staticmethod\n",
        "  def print_data():\n",
        "    print(f\"Name : {PersonSingleton.__instance.name} Age: {PersonSingleton.__instance.age}\")\n",
        "\n",
        "p = PersonSingleton('Anurag', 11)\n",
        "print(p)\n",
        "p.print_data()\n",
        "\n",
        "# p2 = PersonSingleton(\"Anand\", 12) # Exception: Singleton Class cant be re-instantiate\n",
        "p2 = PersonSingleton.get_instance()\n",
        "print(p2) # look at memory address it always refers to same instantiation\n",
        "print(p.name)\n",
        "p2.print_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQbtKFrG-12X",
        "outputId": "33e4af52-12e2-434b-df00-6523f9d965f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.PersonSingleton object at 0x79c030504070>\n",
            "Name : Anurag Age: 11\n",
            "<__main__.PersonSingleton object at 0x79c030504070>\n",
            "Anurag\n",
            "Name : Anurag Age: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coposite Design Pattern\n",
        "\n",
        "- Multiple class inherit from same Interface\n",
        "-"
      ],
      "metadata": {
        "id": "1TLrXyFXDtyz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7y4azGt9M5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deamon"
      ],
      "metadata": {
        "id": "aHZkrbtPFmDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generics\n"
      ],
      "metadata": {
        "id": "ueVQNwDLICVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypeVar, Generic\n",
        "\n",
        "T = TypeVar('T')\n",
        "print(f\" {type(T)} : {T}\")\n",
        "\n",
        "\n",
        "class Stack(Generic[T]):\n",
        "  def __init__(self)->None:\n",
        "    self.items : list[T] = []\n",
        "\n",
        "  def push(self, item:T)->None:\n",
        "    self.items.append(item)\n",
        "\n",
        "  def pop(self)->T:\n",
        "    return self.items.pop()\n",
        "\n",
        "  def empty(self)->bool:\n",
        "    return not self.items()\n",
        "\n",
        "  def print_stack(self):\n",
        "    print(self.items)\n",
        "# The Stack class can be used to represent a stack of any type: Stack[int], Stack[tuple[int, str]]\n",
        "\n",
        "stack = Stack()\n",
        "print(type(Stack))\n",
        "print(type(stack))\n",
        "\n",
        "stack.push(2)\n",
        "stack.push(5)\n",
        "stack.pop()\n",
        "stack.pop()\n",
        "stack.push('y ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPVi2pbMIGkz",
        "outputId": "afb2aff9-b7fb-4625-8209-0a8af82dabe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <class 'typing.TypeVar'> : ~T\n",
            "<class 'type'>\n",
            "<class '__main__.Stack'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stack.print_stack()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBYB41ZnIB9m",
        "outputId": "25605a07-2c44-47fc-c1c3-417a5d76557b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['y ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closures\n",
        "- A Closure in Python is a function object that remembers values in enclosing scopes even if they are not present in memory.\n"
      ],
      "metadata": {
        "id": "7wryBKS4KVKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outerFunction(text):\n",
        "  def innerFunction():\n",
        "    print(text)\n",
        "  return innerFunction\n",
        "\n",
        "myFunc = outerFunction('Anurag')\n",
        "print(myFunc)\n",
        "myFunc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpD9eXF4KbBt",
        "outputId": "08d5483a-ad56-4eba-db78-a246cf3b00af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function outerFunction.<locals>.innerFunction at 0x79c0304e2b00>\n",
            "Anurag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As Python closures are used as callback functions, they provide some sort of data hiding. This helps us to reduce the use of global variables.\n",
        "import logging\n",
        "logging.basicConfig(filename = 'sample_data/example.log', level=logging.INFO)\n",
        "\n",
        "def logger(func):\n",
        "  def log_func(*args):\n",
        "    logging.info(f'Running {func.__name__} wiht argumets {args}')\n",
        "\n",
        "    print(func(*args))\n",
        "  return log_func\n",
        "\n",
        "def add(x, y):\n",
        "  return x + y\n",
        "\n",
        "def sub(x, y):\n",
        "  return x - y\n",
        "\n",
        "add_logger = logger(add)\n",
        "sub_logger = logger(sub)\n",
        "\n",
        "add_logger(3, 3)\n",
        "add_logger(4, 5)\n",
        "\n",
        "sub_logger(3, 3)\n",
        "sub_logger(4, 5)\n",
        "sub_logger.__closure__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1sRnccfKUuV",
        "outputId": "b18e28e7-48fa-40b3-a124-4b5822cae217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "9\n",
            "0\n",
            "-1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<cell at 0x79c0304c72b0: function object at 0x79c0304e0430>,)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls sample_data/"
      ],
      "metadata": {
        "id": "iIUcv7bULrvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6457af-7d61-47ad-a2e1-7399b3e178b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32manscombe.json\u001b[0m*               california_housing_train.csv  mnist_train_small.csv\n",
            "california_housing_test.csv  mnist_test.csv                \u001b[01;32mREADME.md\u001b[0m*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Class\n",
        "- This module provides a decorator and functions for automatically adding generated special methods such as __init__() and __repr__() to user-defined classes\n"
      ],
      "metadata": {
        "id": "Z5ERPRu0U10p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class InventoryItem:\n",
        "  \"\"\" Class for keeping track of items. \"\"\"\n",
        "  name : str\n",
        "  unit_price : float\n",
        "  quantity_on_hand : int = 0\n",
        "\n",
        "  def total_cost(self) -> float:\n",
        "    return self.unit_price * self.quantity_on_hand\n",
        "\n",
        "obj = InventoryItem(name = \"Anurag\", unit_price = 45.44, quantity_on_hand = 5.8)\n",
        "obj.total_cost()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EmD4XFkU5n2",
        "outputId": "341acf49-ba6b-45df-e6f4-869086ec00f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263.55199999999996"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "LpJxINgfSF8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Testing\n",
        "\n"
      ],
      "metadata": {
        "id": "NvtoJRNCgOUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "dir(unittest.TestCase)"
      ],
      "metadata": {
        "id": "0riEQzr_Mjev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426ef2f8-4ce8-42cb-9d97-4c2ee1b9a7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_addExpectedFailure',\n",
              " '_addSkip',\n",
              " '_addUnexpectedSuccess',\n",
              " '_baseAssertEqual',\n",
              " '_callCleanup',\n",
              " '_callSetUp',\n",
              " '_callTearDown',\n",
              " '_callTestMethod',\n",
              " '_deprecate',\n",
              " '_diffThreshold',\n",
              " '_feedErrorsToResult',\n",
              " '_formatMessage',\n",
              " '_getAssertEqualityFunc',\n",
              " '_truncateMessage',\n",
              " 'addClassCleanup',\n",
              " 'addCleanup',\n",
              " 'addTypeEqualityFunc',\n",
              " 'assertAlmostEqual',\n",
              " 'assertAlmostEquals',\n",
              " 'assertCountEqual',\n",
              " 'assertDictContainsSubset',\n",
              " 'assertDictEqual',\n",
              " 'assertEqual',\n",
              " 'assertEquals',\n",
              " 'assertFalse',\n",
              " 'assertGreater',\n",
              " 'assertGreaterEqual',\n",
              " 'assertIn',\n",
              " 'assertIs',\n",
              " 'assertIsInstance',\n",
              " 'assertIsNone',\n",
              " 'assertIsNot',\n",
              " 'assertIsNotNone',\n",
              " 'assertLess',\n",
              " 'assertLessEqual',\n",
              " 'assertListEqual',\n",
              " 'assertLogs',\n",
              " 'assertMultiLineEqual',\n",
              " 'assertNoLogs',\n",
              " 'assertNotAlmostEqual',\n",
              " 'assertNotAlmostEquals',\n",
              " 'assertNotEqual',\n",
              " 'assertNotEquals',\n",
              " 'assertNotIn',\n",
              " 'assertNotIsInstance',\n",
              " 'assertNotRegex',\n",
              " 'assertNotRegexpMatches',\n",
              " 'assertRaises',\n",
              " 'assertRaisesRegex',\n",
              " 'assertRaisesRegexp',\n",
              " 'assertRegex',\n",
              " 'assertRegexpMatches',\n",
              " 'assertSequenceEqual',\n",
              " 'assertSetEqual',\n",
              " 'assertTrue',\n",
              " 'assertTupleEqual',\n",
              " 'assertWarns',\n",
              " 'assertWarnsRegex',\n",
              " 'assert_',\n",
              " 'countTestCases',\n",
              " 'debug',\n",
              " 'defaultTestResult',\n",
              " 'doClassCleanups',\n",
              " 'doCleanups',\n",
              " 'fail',\n",
              " 'failIf',\n",
              " 'failIfAlmostEqual',\n",
              " 'failIfEqual',\n",
              " 'failUnless',\n",
              " 'failUnlessAlmostEqual',\n",
              " 'failUnlessEqual',\n",
              " 'failUnlessRaises',\n",
              " 'failureException',\n",
              " 'id',\n",
              " 'longMessage',\n",
              " 'maxDiff',\n",
              " 'run',\n",
              " 'setUp',\n",
              " 'setUpClass',\n",
              " 'shortDescription',\n",
              " 'skipTest',\n",
              " 'subTest',\n",
              " 'tearDown',\n",
              " 'tearDownClass']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestExample(unittest.TestCase):\n",
        "  def test_assertion(self):\n",
        "    self.assertEquals(\"anurag\", 'anurag')\n",
        "    self.assertEqual(2, 2)\n",
        "    self.assertAlmostEqual(2.2, 2.2)\n",
        "    self.assertNotEqual(2, 3)\n",
        "\n",
        "unittest.main(argv=[''], verbosity=3, exit = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "281bUeWOg_t9",
        "outputId": "1bc2b983-4cfc-4906-ad32-ffa9682f5fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_assertion (__main__.TestExample) ... <ipython-input-27-038aef32f1bd>:3: DeprecationWarning: Please use assertEqual instead.\n",
            "  self.assertEquals(\"anurag\", 'anurag')\n",
            "ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x79c0304c74c0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert \"anurag\" == \"anurag\" # Not useful when performed standalone\n",
        "\n",
        "# When using -o will remove assert statement while debugging the code in running; thats the reason not to use plain assert in production\n",
        "!python --help | grep \"assert\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4eIBGTEhl4R",
        "outputId": "36470305-6a75-4e20-d767-a681c9ab0b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-O     : remove assert and __debug__-dependent statements; add .opt-1 before\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M27pWnx7juuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Test can be funtion or class\n",
        " - Convention for pytest or unittest layout\n",
        " > - Directory need to be \"tests\" plural and then can add anything unit, funtional, database or other menaningful names\n",
        " > - Files should be prefixed with 'test'\n",
        " > - Test function should be prefixed with test_\n",
        " > - Test Class should be prefixed with Test\n",
        "\n"
      ],
      "metadata": {
        "id": "ipS8p20vkUHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_code = \"\"\"\n",
        "def test_passes():\n",
        "  assert True\n",
        "\n",
        "def test_fails():\n",
        "  assert False\n",
        "\"\"\"\n",
        "\n",
        "# Open a file named 'test_example.py' in write mode ('w') (convention followed)\n",
        "with open('test_example.py', 'w') as file:\n",
        "    # Write the corrected code to the file\n",
        "    file.write(test_code)\n",
        "\n",
        "non_test_code = \"\"\"\n",
        "def test_simple():\n",
        "  assert True\n",
        "\n",
        "def test_fails():\n",
        "  assert True\n",
        "\"\"\"\n",
        "\n",
        "# Open a file named 'non_test_example.py' in write mode ('w') (convention not followed)\n",
        "# remove prefix \"non_\" to check test collection\n",
        "with open('non_test_example2.py', 'w') as file:\n",
        "    # Write the corrected code to the file\n",
        "    file.write(non_test_code)\n"
      ],
      "metadata": {
        "id": "pFBXfievmrJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_wog0SVmpdL",
        "outputId": "d8275621-b3f7-4591-cf59-450082c69761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28\n",
            "drwxr-xr-x 1 root root 4096 Jul 23 08:34 .\n",
            "drwxr-xr-x 1 root root 4096 Jul 23 08:32 ..\n",
            "drwxr-xr-x 4 root root 4096 Jul 19 20:30 .config\n",
            "-rw-r--r-- 1 root root   83 Jul 23 08:34 -f\n",
            "-rw-r--r-- 1 root root   67 Jul 23 08:34 non_test_example2.py\n",
            "drwxr-xr-x 1 root root 4096 Jul 19 20:31 sample_data\n",
            "-rw-r--r-- 1 root root   68 Jul 23 08:34 test_example.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function test\n",
        "# test_exmaples.py (naming convention)\n",
        "# non_test_examples.py (non naming convention)\n",
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxelYOUplAxH",
        "outputId": "6ff6f4ba-8214-402a-af1d-4ec4141304fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test_example.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                           [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m____________________________________________ test_fails ____________________________________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_fails\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_example.py\u001b[0m:6: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_example.py::\u001b[1mtest_fails\u001b[0m - assert False\n",
            "\u001b[31m=================================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.18s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest non_test_example.py # explicitly performed not collected automaticaly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "088YRSgNjP0f",
        "outputId": "b1e0fb92-c97c-4eb5-9af6-9aeddbcf4946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 0 items                                                                                  \u001b[0m\n",
            "\n",
            "\u001b[33m====================================== \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =======================================\u001b[0m\n",
            "\u001b[31mERROR: file or directory not found: non_test_example.py\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytest with parameters"
      ],
      "metadata": {
        "id": "YABNFCwQpPJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_dict = \"\"\"\n",
        "def test_long_dict():\n",
        "  result  = {'key': 'value', 'lastname': 'anand', 'firstname': 'anurag'}\n",
        "  expected = {'key': 'value', 'lastnme': 'anand', 'firstname': 'anurag'}\n",
        "  assert result == expected\n",
        "\"\"\"\n",
        "# test_long_dict.py\n",
        "with open(\"test_long_dict.py\", 'w') as f:\n",
        "  f.write(long_dict)\n",
        "!pytest test_long_dict.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSb-jnzXpR3_",
        "outputId": "7ff34523-76e2-4f4c-ecff-c2ee55987102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test_long_dict.py \u001b[31mF\u001b[0m\u001b[31m                                                                          [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m__________________________________________ test_long_dict __________________________________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_long_dict\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "      result  = {\u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlastname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfirstname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manurag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}\u001b[90m\u001b[39;49;00m\n",
            "      expected = {\u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlastnme\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfirstname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manurag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m result == expected\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert {'firstname':...ame': 'anand'} == {'firstname':...nme': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Omitting 2 identical items, use -vv to show\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Left contains 1 more item:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'lastname': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Right contains 1 more item:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'lastnme': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Use -v to get more diff\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_long_dict.py\u001b[0m:5: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_long_dict.py::\u001b[1mtest_long_dict\u001b[0m - AssertionError: assert {'firstname':...ame': 'anand'} == {'firstname':...nme': 'anand'}\n",
            "\u001b[31m======================================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.15s\u001b[0m\u001b[31m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# running full diff engine\n",
        "!pytest -vv test_long_dict.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RG-ESt_p9wr",
        "outputId": "60b2db0b-afaf-4b4e-fb71-b424b67e6f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test_long_dict.py::test_long_dict \u001b[31mFAILED\u001b[0m\u001b[31m                                                     [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m__________________________________________ test_long_dict __________________________________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_long_dict\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "      result  = {\u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlastname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfirstname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manurag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}\u001b[90m\u001b[39;49;00m\n",
            "      expected = {\u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlastnme\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfirstname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manurag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m result == expected\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert {'key': 'value', 'lastname': 'anand', 'firstname': 'anurag'} == {'key': 'value', 'lastnme': 'anand', 'firstname': 'anurag'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Common items:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'firstname': 'anurag', 'key': 'value'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Left contains 1 more item:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'lastname': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Right contains 1 more item:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'lastnme': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Full diff:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       - {'firstname': 'anurag', 'key': 'value', 'lastnme': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + {'firstname': 'anurag', 'key': 'value', 'lastname': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       ?                                               +\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_long_dict.py\u001b[0m:5: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_long_dict.py::\u001b[1mtest_long_dict\u001b[0m - AssertionError: assert {'key': 'value', 'lastname': 'anand', 'firstname': 'anurag'} == {'key': ...\n",
            "\u001b[31m======================================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.18s\u001b[0m\u001b[31m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_test = \"\"\"\n",
        "import pytest\n",
        "\n",
        "def str_to_bool(val):\n",
        "  true_vals = ['yes', 'y', '']\n",
        "  false_vals = ['no', 'n']\n",
        "  try:\n",
        "    val = val.lower()\n",
        "  except AttributeError:\n",
        "    val = str(val).lower()\n",
        "  if val in true_vals:\n",
        "    return True\n",
        "  elif val in false_vals:\n",
        "    return False\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid boolean value: {val}\")\n",
        "\n",
        "@pytest.mark.parametrize('value', ['N', 'y', 'yes', ''])\n",
        "def test_is_true(value):\n",
        "  result = str_to_bool(value)\n",
        "  assert result is True\n",
        "\n",
        "\"\"\"\n",
        "with open('test_parameter.py', 'w') as f:\n",
        "  f.write(parameter_test)\n",
        "\n",
        "!pytest -v test_parameter.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfQ6THBIq78F",
        "outputId": "a6557b20-fb07-443f-c537-713340d12cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 4 items                                                                                  \u001b[0m\n",
            "\n",
            "test_parameter.py::test_is_true[N] \u001b[31mFAILED\u001b[0m\u001b[31m                                                    [ 25%]\u001b[0m\n",
            "test_parameter.py::test_is_true[y] \u001b[32mPASSED\u001b[0m\u001b[31m                                                    [ 50%]\u001b[0m\n",
            "test_parameter.py::test_is_true[yes] \u001b[32mPASSED\u001b[0m\u001b[31m                                                  [ 75%]\u001b[0m\n",
            "test_parameter.py::test_is_true[] \u001b[32mPASSED\u001b[0m\u001b[31m                                                     [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_________________________________________ test_is_true[N] __________________________________________\u001b[0m\n",
            "\n",
            "value = 'N'\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33my\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_is_true\u001b[39;49;00m(value):\u001b[90m\u001b[39;49;00m\n",
            "      result = str_to_bool(value)\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m result \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     assert False is True\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_parameter.py\u001b[0m:21: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_parameter.py::\u001b[1mtest_is_true[N]\u001b[0m - assert False is True\n",
            "\u001b[31m=================================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.17s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --pdb test_parameter.py"
      ],
      "metadata": {
        "id": "Zun2t6w4ENNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c63a46-bc5c-488a-99fd-2e1cd911f32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 4 items                                                                                  \u001b[0m\n",
            "\n",
            "test_parameter.py \u001b[31mF\u001b[0m\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "value = 'N'\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33my\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_is_true\u001b[39;49;00m(value):\u001b[90m\u001b[39;49;00m\n",
            "      result = str_to_bool(value)\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m result \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     assert False is True\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_parameter.py\u001b[0m:21: AssertionError\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "> /content/test_parameter.py(21)test_is_true()\n",
            "-> assert result is True\n",
            "(Pdb) exit\n",
            "\n",
            "\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_parameter.py::\u001b[1mtest_is_true[N]\u001b[0m - assert False is True\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!! _pytest.outcomes.Exit: Quitting debugger !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\u001b[31m======================================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 29.57s\u001b[0m\u001b[31m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --help"
      ],
      "metadata": {
        "id": "fdTUY9M-ERcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbee90d-ce2e-413b-fd5b-8f01e42db842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
            "\n",
            "positional arguments:\n",
            "  file_or_dir\n",
            "\n",
            "general:\n",
            "  -k EXPRESSION         Only run tests which match the given substring expression. An expression is\n",
            "                        a Python evaluatable expression where all names are substring-matched\n",
            "                        against test names and their parent classes. Example: -k 'test_method or\n",
            "                        test_other' matches all test functions and classes whose name contains\n",
            "                        'test_method' or 'test_other', while -k 'not test_method' matches those that\n",
            "                        don't contain 'test_method' in their names. -k 'not test_method and not\n",
            "                        test_other' will eliminate the matches. Additionally keywords are matched to\n",
            "                        classes and functions containing extra names in their\n",
            "                        'extra_keyword_matches' set, as well as functions which have names assigned\n",
            "                        directly to them. The matching is case-insensitive.\n",
            "  -m MARKEXPR           Only run tests matching given mark expression. For example: -m 'mark1 and\n",
            "                        not mark2'.\n",
            "  --markers             show markers (builtin, plugin and per-project ones).\n",
            "  -x, --exitfirst       Exit instantly on first error or failed test\n",
            "  --fixtures, --funcargs\n",
            "                        Show available fixtures, sorted by plugin appearance (fixtures with leading\n",
            "                        '_' are only shown with '-v')\n",
            "  --fixtures-per-test   Show fixtures per test\n",
            "  --pdb                 Start the interactive Python debugger on errors or KeyboardInterrupt\n",
            "  --pdbcls=modulename:classname\n",
            "                        Specify a custom interactive Python debugger for use with --pdb.For example:\n",
            "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
            "  --trace               Immediately break when running each test\n",
            "  --capture=method      Per-test capturing method: one of fd|sys|no|tee-sys\n",
            "  -s                    Shortcut for --capture=no\n",
            "  --runxfail            Report the results of xfail tests as if they were not marked\n",
            "  --lf, --last-failed   Rerun only the tests that failed at the last run (or all if none failed)\n",
            "  --ff, --failed-first  Run all tests, but run the last failures first. This may re-order tests and\n",
            "                        thus lead to repeated fixture setup/teardown.\n",
            "  --nf, --new-first     Run tests from new files first, then the rest of the tests sorted by file\n",
            "                        mtime\n",
            "  --cache-show=[CACHESHOW]\n",
            "                        Show cache contents, don't perform collection or tests. Optional argument:\n",
            "                        glob (default: '*').\n",
            "  --cache-clear         Remove all cache contents at start of test run\n",
            "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
            "                        With ``--lf``, determines whether to execute tests when there are no\n",
            "                        previously (known) failures or when no cached ``lastfailed`` data was found.\n",
            "                        ``all`` (the default) runs the full test suite again. ``none`` just emits a\n",
            "                        message about no known failures and exits successfully.\n",
            "  --sw, --stepwise      Exit on test failure and continue from last failing test next time\n",
            "  --sw-skip, --stepwise-skip\n",
            "                        Ignore the first failing test but stop on the next failing test. Implicitly\n",
            "                        enables --stepwise.\n",
            "\n",
            "Reporting:\n",
            "  --durations=N         Show N slowest setup/test durations (N=0 for all)\n",
            "  --durations-min=N     Minimal duration in seconds for inclusion in slowest list. Default: 0.005.\n",
            "  -v, --verbose         Increase verbosity\n",
            "  --no-header           Disable header\n",
            "  --no-summary          Disable summary\n",
            "  -q, --quiet           Decrease verbosity\n",
            "  --verbosity=VERBOSE   Set verbosity. Default: 0.\n",
            "  -r chars              Show extra test summary info as specified by chars: (f)ailed, (E)rror,\n",
            "                        (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll\n",
            "                        except passed (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
            "                        --disable-warnings), 'N' can be used to reset the list. (default: 'fE').\n",
            "  --disable-warnings, --disable-pytest-warnings\n",
            "                        Disable warnings summary\n",
            "  -l, --showlocals      Show locals in tracebacks (disabled by default)\n",
            "  --no-showlocals       Hide locals in tracebacks (negate --showlocals passed through addopts)\n",
            "  --tb=style            Traceback print mode (auto/long/short/line/native/no)\n",
            "  --show-capture={no,stdout,stderr,log,all}\n",
            "                        Controls how captured stdout/stderr/log is shown on failed tests. Default:\n",
            "                        all.\n",
            "  --full-trace          Don't cut any tracebacks (default is to cut)\n",
            "  --color=color         Color terminal output (yes/no/auto)\n",
            "  --code-highlight={yes,no}\n",
            "                        Whether code should be highlighted (only if --color is also enabled).\n",
            "                        Default: yes.\n",
            "  --pastebin=mode       Send failed|all info to bpaste.net pastebin service\n",
            "  --junit-xml=path      Create junit-xml style report file at given path\n",
            "  --junit-prefix=str    Prepend prefix to classnames in junit-xml output\n",
            "\n",
            "pytest-warnings:\n",
            "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
            "                        Set which warnings to report, see -W option of Python itself\n",
            "  --maxfail=num         Exit after first num failures or errors\n",
            "  --strict-config       Any warnings encountered while parsing the `pytest` section of the\n",
            "                        configuration file raise errors\n",
            "  --strict-markers      Markers not registered in the `markers` section of the configuration file\n",
            "                        raise errors\n",
            "  --strict              (Deprecated) alias to --strict-markers\n",
            "  -c FILE, --config-file=FILE\n",
            "                        Load configuration from `FILE` instead of trying to locate one of the\n",
            "                        implicit configuration files.\n",
            "  --continue-on-collection-errors\n",
            "                        Force test execution even if collection errors occur\n",
            "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path: 'root_dir',\n",
            "                        './root_dir', 'root_dir/another_dir/'; absolute path: '/home/user/root_dir';\n",
            "                        path with variables: '$HOME/root_dir'.\n",
            "\n",
            "collection:\n",
            "  --collect-only, --co  Only collect tests, don't execute them\n",
            "  --pyargs              Try to interpret all arguments as Python packages\n",
            "  --ignore=path         Ignore path during collection (multi-allowed)\n",
            "  --ignore-glob=path    Ignore path pattern during collection (multi-allowed)\n",
            "  --deselect=nodeid_prefix\n",
            "                        Deselect item (via node id prefix) during collection (multi-allowed)\n",
            "  --confcutdir=dir      Only load conftest.py's relative to specified dir\n",
            "  --noconftest          Don't load any conftest.py files\n",
            "  --keep-duplicates     Keep duplicate tests\n",
            "  --collect-in-virtualenv\n",
            "                        Don't ignore tests in a local virtualenv directory\n",
            "  --import-mode={prepend,append,importlib}\n",
            "                        Prepend/append to sys.path when importing test modules and conftest files.\n",
            "                        Default: prepend.\n",
            "  --doctest-modules     Run doctests in all .py modules\n",
            "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
            "                        Choose another output format for diffs on doctest failure\n",
            "  --doctest-glob=pat    Doctests file matching pattern, default: test*.txt\n",
            "  --doctest-ignore-import-errors\n",
            "                        Ignore doctest ImportErrors\n",
            "  --doctest-continue-on-failure\n",
            "                        For a given doctest, continue to run after the first failure\n",
            "\n",
            "test session debugging and configuration:\n",
            "  --basetemp=dir        Base temporary directory for this test run. (Warning: this directory is\n",
            "                        removed if it exists.)\n",
            "  -V, --version         Display pytest version and information about plugins. When given twice, also\n",
            "                        display information about plugins.\n",
            "  -h, --help            Show help message and configuration info\n",
            "  -p name               Early-load given plugin module name or entry point (multi-allowed). To avoid\n",
            "                        loading of plugins, use the `no:` prefix, e.g. `no:doctest`.\n",
            "  --trace-config        Trace considerations of conftest.py files\n",
            "  --debug=[DEBUG_FILE_NAME]\n",
            "                        Store internal tracing debug information in this log file. This file is\n",
            "                        opened with 'w' and truncated as a result, care advised. Default:\n",
            "                        pytestdebug.log.\n",
            "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
            "                        Override ini option with \"option=value\" style, e.g. `-o xfail_strict=True -o\n",
            "                        cache_dir=cache`.\n",
            "  --assert=MODE         Control assertion debugging tools.\n",
            "                        'plain' performs no assertion debugging.\n",
            "                        'rewrite' (the default) rewrites assert statements in test modules on import\n",
            "                        to provide assert expression information.\n",
            "  --setup-only          Only setup fixtures, do not execute tests\n",
            "  --setup-show          Show setup of fixtures while executing tests\n",
            "  --setup-plan          Show what fixtures and tests would be executed but don't execute anything\n",
            "\n",
            "logging:\n",
            "  --log-level=LEVEL     Level of messages to catch/display. Not set by default, so it depends on the\n",
            "                        root/parent log handler's effective level, where it is \"WARNING\" by default.\n",
            "  --log-format=LOG_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-date-format=LOG_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-cli-level=LOG_CLI_LEVEL\n",
            "                        CLI logging level\n",
            "  --log-cli-format=LOG_CLI_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-file=LOG_FILE   Path to a file when logging will be written to\n",
            "  --log-file-level=LOG_FILE_LEVEL\n",
            "                        Log file logging level\n",
            "  --log-file-format=LOG_FILE_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-auto-indent=LOG_AUTO_INDENT\n",
            "                        Auto-indent multiline messages passed to the logging module. Accepts\n",
            "                        true|on, false|off or an integer.\n",
            "  --log-disable=LOGGER_DISABLE\n",
            "                        Disable a logger by name. Can be passed multiple times.\n",
            "\n",
            "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:\n",
            "\n",
            "  markers (linelist):   Markers for test functions\n",
            "  empty_parameter_set_mark (string):\n",
            "                        Default marker for empty parametersets\n",
            "  norecursedirs (args): Directory patterns to avoid for recursion\n",
            "  testpaths (args):     Directories to search for tests when no files or directories are given on\n",
            "                        the command line\n",
            "  filterwarnings (linelist):\n",
            "                        Each line specifies a pattern for warnings.filterwarnings. Processed after\n",
            "                        -W/--pythonwarnings.\n",
            "  usefixtures (args):   List of default fixtures to be used with this project\n",
            "  python_files (args):  Glob-style file patterns for Python test module discovery\n",
            "  python_classes (args):\n",
            "                        Prefixes or glob names for Python test class discovery\n",
            "  python_functions (args):\n",
            "                        Prefixes or glob names for Python test function and method discovery\n",
            "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
            "                        Disable string escape non-ASCII characters, might cause unwanted side\n",
            "                        effects(use at your own risk)\n",
            "  console_output_style (string):\n",
            "                        Console output: \"classic\", or with additional progress information\n",
            "                        (\"progress\" (percentage) | \"count\" | \"progress-even-when-capture-no\" (forces\n",
            "                        progress even when capture=no)\n",
            "  xfail_strict (bool):  Default for the strict parameter of xfail markers when not given explicitly\n",
            "                        (default: False)\n",
            "  tmp_path_retention_count (string):\n",
            "                        How many sessions should we keep the `tmp_path` directories, according to\n",
            "                        `tmp_path_retention_policy`.\n",
            "  tmp_path_retention_policy (string):\n",
            "                        Controls which directories created by the `tmp_path` fixture are kept\n",
            "                        around, based on test outcome. (all/failed/none)\n",
            "  enable_assertion_pass_hook (bool):\n",
            "                        Enables the pytest_assertion_pass hook. Make sure to delete any previously\n",
            "                        generated pyc cache files.\n",
            "  junit_suite_name (string):\n",
            "                        Test suite name for JUnit report\n",
            "  junit_logging (string):\n",
            "                        Write captured log messages to JUnit report: one of\n",
            "                        no|log|system-out|system-err|out-err|all\n",
            "  junit_log_passing_tests (bool):\n",
            "                        Capture log information for passing tests to JUnit report:\n",
            "  junit_duration_report (string):\n",
            "                        Duration time to report: one of total|call\n",
            "  junit_family (string):\n",
            "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
            "  doctest_optionflags (args):\n",
            "                        Option flags for doctests\n",
            "  doctest_encoding (string):\n",
            "                        Encoding used for doctest files\n",
            "  cache_dir (string):   Cache directory path\n",
            "  log_level (string):   Default value for --log-level\n",
            "  log_format (string):  Default value for --log-format\n",
            "  log_date_format (string):\n",
            "                        Default value for --log-date-format\n",
            "  log_cli (bool):       Enable log display during test run (also known as \"live logging\")\n",
            "  log_cli_level (string):\n",
            "                        Default value for --log-cli-level\n",
            "  log_cli_format (string):\n",
            "                        Default value for --log-cli-format\n",
            "  log_cli_date_format (string):\n",
            "                        Default value for --log-cli-date-format\n",
            "  log_file (string):    Default value for --log-file\n",
            "  log_file_level (string):\n",
            "                        Default value for --log-file-level\n",
            "  log_file_format (string):\n",
            "                        Default value for --log-file-format\n",
            "  log_file_date_format (string):\n",
            "                        Default value for --log-file-date-format\n",
            "  log_auto_indent (string):\n",
            "                        Default value for --log-auto-indent\n",
            "  pythonpath (paths):   Add paths to sys.path\n",
            "  faulthandler_timeout (string):\n",
            "                        Dump the traceback of all threads if a test takes more than TIMEOUT seconds\n",
            "                        to finish\n",
            "  addopts (args):       Extra command line options\n",
            "  minversion (string):  Minimally required pytest version\n",
            "  required_plugins (args):\n",
            "                        Plugins that must be present for pytest to run\n",
            "\n",
            "Environment variables:\n",
            "  PYTEST_ADDOPTS           Extra command line options\n",
            "  PYTEST_PLUGINS           Comma-separated plugins to load during startup\n",
            "  PYTEST_DISABLE_PLUGIN_AUTOLOAD Set to disable plugin auto-loading\n",
            "  PYTEST_DEBUG             Set to enable debug tracing of pytest's internals\n",
            "\n",
            "\n",
            "to see available markers type: pytest --markers\n",
            "to see available fixtures type: pytest --fixtures\n",
            "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrX3EEsV3bTu",
        "outputId": "b8fc5d23-18d9-482c-cc31-028158da0116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 7 items                                                                                  \u001b[0m\n",
            "\n",
            "test_example.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                           [ 28%]\u001b[0m\n",
            "test_long_dict.py \u001b[31mF\u001b[0m\u001b[31m                                                                          [ 42%]\u001b[0m\n",
            "test_parameter.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                       [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m____________________________________________ test_fails ____________________________________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_fails\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_example.py\u001b[0m:6: AssertionError\n",
            "\u001b[31m\u001b[1m__________________________________________ test_long_dict __________________________________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_long_dict\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "      result  = {\u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlastname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfirstname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manurag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}\u001b[90m\u001b[39;49;00m\n",
            "      expected = {\u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlastnme\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfirstname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33manurag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m result == expected\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert {'firstname':...ame': 'anand'} == {'firstname':...nme': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Omitting 2 identical items, use -vv to show\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Left contains 1 more item:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'lastname': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Right contains 1 more item:\u001b[0m\n",
            "\u001b[1m\u001b[31mE       {'lastnme': 'anand'}\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Use -v to get more diff\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_long_dict.py\u001b[0m:5: AssertionError\n",
            "\u001b[31m\u001b[1m_________________________________________ test_is_true[N] __________________________________________\u001b[0m\n",
            "\n",
            "value = 'N'\n",
            "\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33my\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_is_true\u001b[39;49;00m(value):\u001b[90m\u001b[39;49;00m\n",
            "      result = str_to_bool(value)\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m result \u001b[95mis\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     assert False is True\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_parameter.py\u001b[0m:21: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_example.py::\u001b[1mtest_fails\u001b[0m - assert False\n",
            "\u001b[31mFAILED\u001b[0m test_long_dict.py::\u001b[1mtest_long_dict\u001b[0m - AssertionError: assert {'firstname':...ame': 'anand'} == {'firstname':...nme': 'anand'}\n",
            "\u001b[31mFAILED\u001b[0m test_parameter.py::\u001b[1mtest_is_true[N]\u001b[0m - assert False is True\n",
            "\u001b[31m=================================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.11s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"pip install pytest-dist\"  for running parallel test with flag \"pytest -n\"\n",
        "# \"pip install nbmake\" for running testcase written in notebook \"pytest --nbmake python.py\""
      ],
      "metadata": {
        "id": "-wa6aLDm3zKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# click (build using optparse instead of argparse)"
      ],
      "metadata": {
        "id": "ZZ4pOvIPvs9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_code = \"\"\"\n",
        "import click\n",
        "@click.command()\n",
        "@click.option('--count', default=1, type=int, help='Number of greetings.')\n",
        "@click.option('--firstname', type=str, prompt='Your name', help='Person firstname')\n",
        "@click.option('--lastname', type=str, prompt='Your last name', help='Person lastname')\n",
        "def hello(count, firstname, lastname):\n",
        "  for i in range(count):\n",
        "    click.echo(f\"Hello {firstname} {lastname}\")\n",
        "\n",
        "if __name__== '__main__':\n",
        "  hello()\n",
        "\"\"\"\n",
        "\n",
        "with open('hello.py', 'w') as f:\n",
        "  f.write(test_code)\n",
        "\n",
        "# !python hello.py # bydefault ask for firstname and lastname\n",
        "!python hello.py --count 5 --firstname anurag --lastname anand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eekrSVNvI6h",
        "outputId": "ce9803cf-2625-4bc1-c884-d124d322b99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello anurag anand\n",
            "Hello anurag anand\n",
            "Hello anurag anand\n",
            "Hello anurag anand\n",
            "Hello anurag anand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python hello.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za7z7nqj4FD2",
        "outputId": "ebfb50b5-0e0e-4bf5-b2fa-e30ca56a9fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hello.py [OPTIONS]\n",
            "\n",
            "Options:\n",
            "  --count INTEGER   Number of greetings.\n",
            "  --firstname TEXT  Person firstname\n",
            "  --lastname TEXT   Person lastname\n",
            "  --help            Show this message and exit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owkY48kh2--C",
        "outputId": "ab7d62c1-0e09-420e-ca8d-f0d2fee7645b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cmake version 3.27.9\n",
            "\n",
            "CMake suite maintained and supported by Kitware (kitware.com/cmake).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "makefile = \"\"\"\n",
        "install:\n",
        "        pip install --upgrade pip &&\\\n",
        "        pip install pytest cmake\n",
        "test:\n",
        "        pytest -vv test_example.py\n",
        "format:\n",
        "        black *py\n",
        "lint:\n",
        "        pylint --disable=R,C test_example.py\n",
        "all: install lint test format\n",
        "\n",
        "cmake_install:\n",
        "        @echo \"CMake install\"\n",
        "\"\"\"\n",
        "with open('Makefile', 'w') as f:\n",
        "  f.write(makefile)\n",
        "\n",
        "!make      install\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQN7c-4R4E4I",
        "outputId": "f33799d1-5cd2-4905-b027-a26a3b464aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Makefile:3: *** missing separator (did you mean TAB instead of 8 spaces?).  Stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kocEv1j-3yh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcqRzbm81XSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJwAMSoA4E0u",
        "outputId": "e355c60a-4100-49fe-c270-83efdd44eb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 48\n",
            "drwxr-xr-x 1 root root 4096 Jul 23 08:59 .\n",
            "drwxr-xr-x 1 root root 4096 Jul 23 08:32 ..\n",
            "drwxr-xr-x 4 root root 4096 Jul 19 20:30 .config\n",
            "-rw-r--r-- 1 root root   83 Jul 23 08:34 -f\n",
            "-rw-r--r-- 1 root root  426 Jul 23 08:56 hello.py\n",
            "-rw-r--r-- 1 root root    0 Jul 23 08:59 Makefile\n",
            "-rw-r--r-- 1 root root   67 Jul 23 08:34 non_test_example2.py\n",
            "drwxr-xr-x 2 root root 4096 Jul 23 08:34 __pycache__\n",
            "drwxr-xr-x 3 root root 4096 Jul 23 08:34 .pytest_cache\n",
            "drwxr-xr-x 1 root root 4096 Jul 19 20:31 sample_data\n",
            "-rw-r--r-- 1 root root   68 Jul 23 08:34 test_example.py\n",
            "-rw-r--r-- 1 root root  197 Jul 23 08:34 test_long_dict.py\n",
            "-rw-r--r-- 1 root root  459 Jul 23 08:34 test_parameter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --pdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9NJcrZI0VKK",
        "outputId": "fcf4d15e-f693-4203-be04-b47daf1df015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 7 items                                                                                  \u001b[0m\n",
            "\n",
            "test_example.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_fails\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_example.py\u001b[0m:6: AssertionError\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "> /content/test_example.py(6)test_fails()\n",
            "-> assert False\n",
            "(Pdb) exit\n",
            "\n",
            "\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_example.py::\u001b[1mtest_fails\u001b[0m - assert False\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!! _pytest.outcomes.Exit: Quitting debugger !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\u001b[31m=================================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 9.50s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}